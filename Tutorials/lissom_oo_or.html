
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>LISSOM Orientation Map &mdash; The Topographica Neural Map Simulator</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/topo.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.98',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/topo.js"></script>
    <link rel="top" title="The Topographica Neural Map Simulator" href="../index.html" />
    <link rel="up" title="Tutorials" href="index.html" />
    <link rel="next" title="SOM Retinotopic Map" href="som_retinotopy.html" />
    <link rel="prev" title="GCAL Orientation Map" href="gcal.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="som_retinotopy.html" title="SOM Retinotopic Map"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="gcal.html" title="GCAL Orientation Map"
             accesskey="P">previous</a> |</li>
<li><a href="../index.html">Home</a></li>
<li><a href="../Downloads/index.html">Downloads</a></li>
<li><a href="index.html">Tutorials</a></li>
<li><a href="../User_Manual/index.html">User Manual</a></li>


<li><ul class="parents">



          <li><a href="index.html" accesskey="U">Tutorials</a> &raquo;</li>

</ul></li>


      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="lissom-orientation-map">
<h1>LISSOM Orientation Map<a class="headerlink" href="#lissom-orientation-map" title="Permalink to this headline">¶</a></h1>
<p>This tutorial shows how to use the <a class="reference external" href="http://topographica.org/">Topographica</a> software package
to explore a simple orientation map simulation using test patterns
and weight plots. This particular example uses a <a class="reference external" href="http://homepages.inf.ed.ac.uk/jbednar/research.html">LISSOM model</a>
cortex. Although we focus on one model in this tutorial,
Topographica provides support for many other models and is easily
extensible for models not yet supported.</p>
<p>This tutorial assumes that you have already followed the
instructions for <a class="reference external" href="../Downloads/index.html">obtaining and installing</a> Topographica. Also, you
will need to generate a saved orientation map network (a .typ file),
which can be done from a Unix or Mac terminal or Windows <a class="reference external" href="../Downloads/win32notes.html">command
prompt</a> by running</p>
<blockquote>
<div><div class="highlight-python"><pre>topographica -a -c "generate_example('lissom_oo_or_10000.typ')"</pre>
</div>
</div></blockquote>
<p>Depending on the speed of your machine, you may want to go get a
snack at this point; on a 3GHz 512MB machine this training process
currently takes from 7-15 minutes (depending on the amount of level
2 cache). When training completes, lissom_oo_or_10000.typ will be
saved in Topographica&#8217;s <a class="reference external" href="../User_Manual/scripts.html#outputpath">output path</a> ready for use in the
tutorial.</p>
<div class="section" id="response-of-an-orientation-map">
<h2>Response of an orientation map<a class="headerlink" href="#response-of-an-orientation-map" title="Permalink to this headline">¶</a></h2>
<p>In this example, we will load a saved network and test its behavior
by presenting different visual input patterns.</p>
<ol class="arabic">
<li><p class="first">First, start the Topographica GUI from a terminal:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">topographica</span> <span class="o">-</span><span class="n">g</span>
</pre></div>
</div>
<p>(Windows users can instead double click on the desktop
Topographica icon.)
This will open the Topographica console:</p>
<div class="figure align-center">
<img alt="Console Window" src="../_images/topographica_console.png" />
</div>
<p>The window and button style will differ on different platforms,
but similar buttons should be provided.</p>
</li>
<li><p class="first">Next, load the saved network by selecting selecting Load snapshot
from the Simulation menu and selecting
<tt class="docutils literal"><span class="pre">lissom_oo_or_10000.typ</span></tt>. This small orientation map simulation
should load in a few seconds, with a 54x54 retina, a 36x36 LGN
(composed of one 36x36 OFF channel sheet, and one 36x36 ON
channel sheet), and a 48x48 V1 with about two million synaptic
weights. The architecture can be viewed in the Model Editor
window (which can be selected from the Simulation menu), but is
also shown below:</p>
<div class="figure align-center">
<img alt="LISSOM network" src="../_images/lissom_network_diagram_oo.png" />
</div>
</li>
<li id="activity-plot"><p class="first">To see how this network responds to a simple visual image, first
open an Activity window from the Plots menu on the Topographica
Console, then select Test pattern from the Simulation menu to get
the Test Pattern window:</p>
<div class="figure align-center">
<img alt="Test Pattern window" src="../_images/test_pattern_oo.png" />
</div>
<p>Then select a Line Pattern generator, and hit Present to present
a horizontal line to the network.</p>
</li>
<li><p class="first">The Activity window should then show the result:</p>
<div class="figure align-center">
<img alt="Response to a line" src="../_images/activity_line_oo.png" />
</div>
<p>This window shows the response for each neural area. For now,
please turn on Strength only; it is usually off by default.</p>
<p>As you move your mouse over the plots, information about the
location of the mouse cursor is displayed in the status bar at
the bottom of the window. For these plots, you can see the
<a class="reference external" href="../User_Manual/space.html#matrix-coords">matrix coordinates</a> (labeled &#8220;Unit&#8221;), <a class="reference external" href="../User_Manual/space.html#sheet-coords">sheet coordinates</a>
(labeled &#8220;Coord&#8221;), and the activity level of the unit currently
under the pointer.</p>
<p>In the Retina plot, each photoreceptor is represented as a pixel
whose shade of gray codes the response level, increasing from
black to white. This pattern is what was specified in the Test
Pattern window. Similarly, locations in the LGN that have an OFF
or ON cell response to this pattern are shown in the LGNOff and
LGNOn plots. At this stage the response level in V1 is also coded
in shades of gray, and the numeric values can be found using the
pointer.</p>
<p>From these plots, you can see that the single line presented on
the retina is edge-detected in the LGN, with ON LGN cells
responding to areas brighter than their surround, and OFF LGN
cells responding to areas darker than their surround. In V1, the
response is patchy, as explained below.</p>
</li>
<li id="connectionfields-plot"><p class="first">To help understand the response patterns in V1, we can look at
the weights to V1 neurons. These weights were learned previously,
as a result of presenting 10000 pairs of oriented Gaussian
patterns at random angles and positions. To plot a single neuron,
select Connection Fields from the Plots menu. This will plot the
synaptic strengths of connections to the neuron in the center of
the cortex (by default):</p>
<div class="figure align-center">
<img alt="Weights of one neuron" src="../_images/unit_weights_0_0_oo.png" />
</div>
<p>Again, for now please turn on Strength only; it is usually off by
default.</p>
<p>The plot shows the afferent weights to V1 (i.e., connections from
the ON and OFF channels of the LGN), followed by the lateral
excitatory and lateral inhibitory weights to that neuron from
nearby neurons in V1. The afferent weights represent the retinal
pattern that would most excite the neuron. For the particular
neuron shown above, the optimal retinal stimulus would be a
short, bright line oriented at about 0 degrees (from 9 o&#8217;clock to
3 o&#8217;clock) in the center of the retina. (Note that the particular
neuron you are viewing may have a different preferred
orientation.)</p>
</li>
<li id="projection-plot"><p class="first">If all neurons had the same weight pattern, the response would
not be patchy &#8211; it would just be a blurred version of the input
(for inputs matching the weight pattern), or blank (for other
inputs). To see what the other neurons look like, select
Projection from the Plots menu, then select LGNOnAfferent from
the drop-down Projection list, followed by the refresh arrow next
to &#8216;Pre plot hooks&#8217;:</p>
<div class="figure align-center">
<img alt="Afferent weights of many neurons" src="../_images/projection_oo.png" />
</div>
<p>This plot shows the afferent weights from the LGN ON sheet for
every fifth neuron in each direction. You can see that most of
the neurons are selective for orientation (not just a circular
spot), and each has a slightly different preferred orientation.
This suggests an explanation for why the response is patchy:
neurons preferring orientations other than the one present on the
retina do not respond. You can also look at the LateralInhibitory
weights instead of LGNOnAfferent; those are patchy as well
because the typical activity patterns are patchy.</p>
</li>
<li id="orientationpreference-plot"><p class="first">To visualize all the neurons at once in experimental animals,
optical imaging experiments measure responses to a variety of
patterns and record the one most effective at stimulating each
neuron. The results of a similar procedure can be viewed by
selecting Plots &gt; Preference Maps &gt; Orientation Preference:</p>
<div class="figure align-center">
<img alt="Orientation map" src="../_images/oo_or_map.png" />
</div>
<p>The Orientation Preference plot is the orientation map for V1 in
this model. Each neuron in the plot is color coded by its
preferred orientation, according to the key shown to the left of
the plot. (If viewing a monochrome printout, see web page for the
colors). Note that phase preference and selectivity are also
displayed in the window, but these are not analyzed here (and are
not shown above).</p>
<p>You can see that nearby neurons have similar orientation
preferences, as found in primate visual cortex. The Orientation
Selectivity plot shows the relative selectivity of each neuron
for orientation on an arbitrary scale; you can see that in this
simulation nearly all neurons became orientation selective. The
Orientation Preference&amp;Selectivity plot shows the two other
Orientation plots combined &#8211; each neuron is colored with its
preferred orientation, and the stronger the selectivity, the
brighter the color. In this case, because the neurons are
strongly selective, the Preference&amp;Selectivity plot is nearly
identical to the Preference plot.</p>
<p>If you want to see what happens during map measurement, you can
watch the procedure as it takes place by enabling visualization.
Edit the &#8216;Pre plot hooks&#8217; (as described in the <a class="reference external" href="../User_Manual/plotting.html#changing-existing-plots">Changing existing
plots section of the User Manual</a>) so
that the <a class="reference external" href="../Reference_Manual/topo.command.analysis.measure_sine_pref-class.html">measure_sine_pref</a> command&#8217;s <tt class="docutils literal"><span class="pre">display</span></tt> parameter
is turned on. Open an Activity window and ensure it has
Auto-Refresh turned on, then press Refresh by the Orientation
Preference window&#8217;s &#8216;Pre plot hooks&#8217;. You will see a series of
sine gratings presented to the network, and can observe the
response each time in the LGN and V1 sheets. When you are done,
press Refresh on the pre-plot hooks in the Activity window to
restore the original activity pattern plots.</p>
</li>
<li><p class="first">Now that we have looked at the orientation map, we can see more
clearly why activation patterns are patchy by coloring each
neuron with its orientation preference. To do this, make sure
that Strength only is now turned <em>off</em> in the Activity window:</p>
<dl class="docutils">
<dt><img alt="Color-coded response to a line" src="../_images/activity_line_oo_or.png" /></dt>
<dd><p class="first last"><img alt="Orientation key" src="../_images/or_key_horiz_transparent.png" /></p>
</dd>
</dl>
<p>Each V1 neuron is now color coded by its orientation, with
brighter colors indicating stronger activation. Additionally, the
status bar beneath the plots now also shows the values of the
separate channels comprising the plot: OrientationPreference
(color), OrientationSelectivity (saturation), and Activity
(brightness).</p>
<p>The color coding allows us to see that the neurons responding are
indeed those that prefer orientations similar to the input
pattern, and that the response is patchy because other nearby
neurons do not respond. To be sure of that, try selecting a line
with a different orientation, and hit present again &#8211; the colors
should be different, and should match the orientation chosen.</p>
</li>
<li><p class="first">If you now turn off Strength only in the Connection Fields
window, you can see that the neuron whose weights we plotted is
located in a patch of neurons with similar orientation
preferences:</p>
<dl class="docutils">
<dt><img alt="Colorized weights of one neuron" src="../_images/unit_weights_0_0_oo_or.png" /></dt>
<dd><p class="first last"><img alt="image3" src="../_images/or_key_horiz_transparent.png" /></p>
</dd>
</dl>
<p>Look at the LateralExcitatory weights, which show that the
neurons near the above neuron are nearly all red, to match its
preferred orientation.</p>
<p>Returning to the Test pattern window, try presenting a vertical
line (orientation of <tt class="docutils literal"><span class="pre">pi/2</span></tt>) and then, in the Activity window,
right click on one of the cyan-colored patches of activity. This
will bring up a menu:</p>
<div class="figure align-center">
<img alt="Right-click menu" src="../_images/lissom_oo_or_activity_rightclick.png" />
</div>
<p>The menu offers operations on different parts of the plot: the
first submenu shows operations available on the single selected
unit, and the second shows operations available on the combined
(visible) plot. The final three submenus show operations
available on each of the separate channels that comprise the
plot.</p>
<p>Here we are interested to see the connection fields of the unit
we selected, so we choose Connection Fields from the Single unit
submenu to get a new plot:</p>
<dl class="docutils">
<dt><img alt="image4" src="../_images/unit_weights_41_24_oo_or.png" /></dt>
<dd><p class="first last"><img alt="image5" src="../_images/or_key_horiz_transparent.png" /></p>
</dd>
</dl>
<p>This time we can see from the LateralExcitatory weights that the
neurons near this one are all colored cyan (i.e., are selective
for vertical).</p>
</li>
<li><p class="first">Right-click menus are available on most plots, and provide a
convenient method of further investigating and understanding the
plots. For instance, on the Orientation Preference window, the
connection fields of units at any location can easily be
visualized, allowing one to see the connection fields of units
around different features of the map.</p>
<p>As another example, an interesting property of orientation maps
measured in animals is that their Fourier spectrums usually show
a ring shape, because the orientations repeat at a constant
spatial frequency in all directions. Selecting Hue channel:
OrientationPreference &gt; Fourier transform from the right-click
menu allows us to see the same is true of the map generated by
the LISSOM network:</p>
<div class="figure align-center">
<img alt="FT of orientation preference map" src="../_images/lissom_oo_or_orpref_ft.png" />
</div>
</li>
<li><p class="first">Now that you have a feel for the various plots, you can try
different input patterns, seeing how the cortex responds to each
one. Just select a Pattern generator, e.g. Gaussian, Disk, or
SineGrating, and then hit Present.</p>
<p>For each Pattern generator, you can change various parameters
that control its size, location, etc.:</p>
<blockquote>
<div><dl class="docutils">
<dt>orientation</dt>
<dd><p class="first last">controls the angle (try pi/4 or -pi/4)</p>
</dd>
<dt>x and y</dt>
<dd><p class="first last">control the position on the retina (try 0 or 0.5)</p>
</dd>
<dt>size</dt>
<dd><p class="first last">controls the overall size of e.g. Gaussians and rings</p>
</dd>
<dt>aspect_ratio</dt>
<dd><p class="first last">controls the ratio between width and height; will be
scaled by the size in both directions</p>
</dd>
<dt>smoothing</dt>
<dd><p class="first last">controls the amount of Gaussian falloff around the edges
of patterns such as rings and lines</p>
</dd>
<dt>scale</dt>
<dd><p class="first last">controls the brightness (try 1.0 for a sine grating).
Note that this relatively simple model is very sensitive
to the scale, and scales higher than about 1.2 will
result in a broad, orientation-unselective response,
while low scales will give no response. More <a class="reference external" href="gcal.html">complex
models</a> (and actual brains!) are less sensitive to the
scale or contrast.</p>
</dd>
<dt>offset</dt>
<dd><p class="first last">is added to every pixel</p>
</dd>
<dt>frequency</dt>
<dd><p class="first last">controls frequency of a sine grating or Gabor</p>
</dd>
<dt>phase</dt>
<dd><p class="first last">controls phase of a sine grating or Gabor</p>
</dd>
<dt>mask_shape</dt>
<dd><p class="first last">allows the pattern to be masked by another pattern (e.g.
try a mask_shape of Disk or Ring with a SineGrating or
UniformRandom pattern). The parameters of the mask_shape
pattern can be edited by right-clicking on it.</p>
</dd>
</dl>
</div></blockquote>
<p>To present photographs, select a Pattern generator of type
FileImage. (You can type the path to an image file of your own
(in e.g. PNG, JPG, TIFF, or PGM format) in the filename box.) For
most photographs you will need to change the scale to something
like 2.0 to see a reasonable response from this model V1, and you
may want to enlarge the image size to look at details. A much
larger, more complicated, and slower map would be required to see
interesting patterns in the response to most images, but even
with this network you may be able to see some
orientation-specific responses to large contours in the image:</p>
<p><img alt="Ellen Arthur" src="../_images/natural_image_oo_or.png" /></p>
<p>Here we have enabled Sheet coords so that each plot will be at
the correct size relative to each other. That way, the location
of a given feature can be compared between images. In this
particular network, the Retina and LGN stages each have an extra
&#8220;buffer&#8221; region around the outside so that no V1 neuron will have
its CF cut off, and the result is that V1 sees only the central
region of the image in the LGN, and the LGN sees only the central
region of the retina. (<a class="reference external" href="../User_Manual/space.html#sheet-coords">Sheet coordinates</a> are normally turned
off because they make the cortical plots smaller, but they can be
very helpful for understanding how the sheets relate to each
other.)</p>
</li>
<li><p class="first">The procedure above allows you to explore the relationship
between the input and the final response after the cortex has
settled due to the lateral connections. If you want to understand
the settling process itself, you can also visualize how the
activity propagates from the retina to the LGN, from the LGN to
V1, and then within V1. To do this, first make sure that there is
an Activity window open, with Auto-refresh enabled. Then go to
the console window and hit &#8220;Step&#8221; repeatedly. After an input is
presented, you will see the activity arrive first in the LGN,
then in V1, and then gradually change within V1. The Step button
moves to the next scheduled event in the simulation, which are at
even multiples of 0.05 for this particular simulation. You can
also type in the specific duration (e.g. 0.05) to move forward
into the &#8220;Run for:&#8221; box, and hit &#8220;Go&#8221; instead.</p>
<p>As explained in the <a class="reference external" href="../User_Manual/time.html">User Manual</a>, this process is controlled by
the network structure and the delays between nodes. For
simplicity, let&#8217;s consider time starting at zero. The first
scheduled event is that the Retina will be asked to draw an input
pattern at time 0.05 (the phase of the <a class="reference external" href="../Reference_Manual/topo.sheet.GeneratorSheet-class.html">GeneratorSheet</a>). Thus
the first visible activity occurs in the Retina, at 0.05. The
Retina is connected to the LGN with a delay of 0.05, and so the
LGN responds at 0.10. The delay from the LGN to V1 is also 0.05,
so V1 is first activated at time 0.15. V1 also has self
connections with a delay of 0.05, and so V1 is then repeatedly
activated every 0.05 timesteps. Eventually, the number of V1
activations reaches a fixed limit for LISSOM (usually about 10
timesteps), and no further events are generated or consumed until
the next input is generated at time 1.05. Thus the default
stepsize of 1.0 lets the user see the results after each input
pattern has been presented and the cortex has come to a steady
state, but results can also be examined at a finer timescale. Be
sure to leave the time clock at an even multiple of 1.0 before
you do anything else, so that the network will be in a
well-defined state. (To do this, just type the fractional part
into the &#8220;Run for:&#8221; box, i.e. 0.95 if the time is currently
10002.05, press &#8220;Go&#8221;, and then change &#8220;Run for:&#8221; to 1.0.)</p>
</li>
</ol>
</div>
<div class="section" id="learning-optional">
<h2>Learning (optional)<a class="headerlink" href="#learning-optional" title="Permalink to this headline">¶</a></h2>
<p>The previous examples all used a network trained previously, without
any plasticity enabled. Many researchers are interested in the
processes of development and plasticity. These processes can be
studied using the LISSOM model in Topographica as follows.</p>
<ol class="arabic">
<li><p class="first">First, quit from any existing simulation, and <a class="reference external" href="../User_Manual/scripts.html#copy-examples">get a copy of the
example files to work with</a> if you do not have them already.
Then start a new run of Topographica:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">topographica</span> <span class="o">-</span><span class="n">g</span>
</pre></div>
</div>
<p>From the Simulation menu, select Run Script. Then from the
<tt class="docutils literal"><span class="pre">models</span></tt> directory, open <tt class="docutils literal"><span class="pre">lissom_oo_or.ty</span></tt>.</p>
</li>
<li><p class="first">Next, open an Activity window and make sure that it has
Auto-refresh enabled. Unless your machine is very slow, also
enable Auto-refresh in a Projection window showing LGNOnAfferent.
On a very fast machine you could even Auto-refresh an Orientation
Preference window (probably practical only if you reduce the
nominal_density of V1).</p>
</li>
<li><p class="first">Now hit Go a few times on the Topographica Console window, each
time looking at the random input(s) and the response to them in
the Activity window. The effect on the network weights of
learning this input can be seen in the Projection window.</p>
</li>
<li><p class="first">With each new input, you may be able to see small changes in the
weights of a few neurons in the LGNOnAfferent array (by peering
closely). If the changes are too subtle for your taste, you can
make each input have an obvious effect by speeding up learning to
a highly implausible level. To do this, open the Model Editor
window, right click on the LGNOnAfferent projection (the
cone-shaped lines from LGNOn to V1), select Properties, and
change Learning Rate from the default 0.48 to 100, press Apply,
and then do the same for the LGNOffAfferent projection. Now each
new pattern generated in a training iteration will nearly wipe
out any existing weights.</p>
</li>
<li><p class="first">For more control over the training inputs, open the Test Pattern
window, select a Pattern generator, e.g. Disk, and other
parameters as desired. Then enable Plastic in that window, and
hit Present. You should again see how this input changes the
weights, and can experiment with different inputs.</p>
</li>
<li><p class="first">Once you have a particular input pattern designed, you can see
how that pattern would affect the cortex over many iterations. To
do so, open a Model Editor window and right click on the Retina&#8217;s
diagram, then select Properties from the resulting menu. In the
Parameters of Retina window that opens, select the pattern type
you want to use for the Input Generator item, and then right
click on that pattern, choose Properties and, in the new window,
modify any of its parameters as you wish. Note that you will
probably want to have dynamic values for certain parameters. For
instance, to have a random orientation for each presentation,
right click on Orientation and select Enter dynamic value. The
slider will disappear from the entry box, and you can type in an
expression such as
<tt class="docutils literal"><span class="pre">numbergen.UniformRandom(lbound=-pi,ubound=pi)</span></tt>. When you have
finished configuring your pattern, press Apply or Close on the
Parameters of Gaussian window. Having now set up the input
generator on the Parameters of Retina window, click Apply or
Close on this too. Now when you press Go on the console window
(assuming Run for is set to 1), you should see your pattern being
presented in the Activity Window.</p>
</li>
<li><p class="first">After a few steps (or to do e.g. 20 steps in a row, change Run
for to 20 and press return) you can plot (or refresh) an
Orientation Preference map to see what sort of orientation map
has developed. (Press the &#8216;Refresh&#8217; button next to the Pre plot
hooks if no plot is visible when first opening the window.
Measuring a new map will usually take about 15 seconds to
complete.) If you&#8217;ve changed the learning rate to a high value,
or haven&#8217;t presented many inputs, the map will not resemble
actual animal maps, but it should still have patches selective
for each orientation.</p>
</li>
<li><p class="first">If you are patient, you can even run a full, more realistic,
simulation with your favorite type of input. To do this, quit and
start again, then change the Retina&#8217;s Input generator as before
via the Model Editor, but make sure not to change the learning
rate this time. Then you can change Run for to 10000 and press Go
to see how a full simulation would work with your new inputs.
Running for 10000 iterations will likely take at least several
minutes for recent machines; if you are less patient, try doing
1000 iterations at a time instead before looking at an
Orientation Preference map.</p>
</li>
<li><p class="first">If you are <em>really</em> patient, you can change the number of units
to something closer to real primate cortex, by quitting and then
restarting with a higher density in V1. To do this, you will need
to specify the example script from the commandline. The path of
the lissom_oo_or.ty script was printed by Topographica in step
1 of this Learning section, but if you are not sure where the
examples are located, you can find out by first running</p>
<div class="highlight-python"><pre>topographica -c "from topo.misc.genexamples import print_examples_dir; print_examples_dir()"</pre>
</div>
<p>Then you can use the path to the example, as well as specifying a
higher cortex density, e.g.</p>
<div class="highlight-python"><pre>topographica -p cortex_density=142 ~/Documents/Topographica/examples/lissom_oo_or.ty -g</pre>
</div>
<p>You&#8217;ll need about a gigabyte of memory and a lot of time, but you
can then step through the simulation as above. The final result
after 10000 iterations (requiring several hours on a 3GHz
machine) should be a much smoother map and neurons that are more
orientation selective. Even so, the overall organization and
function should be similar.</p>
</li>
</ol>
</div>
<div class="section" id="exploring-further">
<h2>Exploring further<a class="headerlink" href="#exploring-further" title="Permalink to this headline">¶</a></h2>
<p>To see how the example works, load the lissom_oo_or.ty file into a
text editor and see how it has been defined, then find the
corresponding Python code for each module and see how that has been
defined.</p>
<p>Topographica comes with additional examples, and more are always
being added. In particular, the above examples work in nearly the
same way with the simpler <tt class="docutils literal"><span class="pre">lissom_or.ty</span></tt> model that has no LGN.
Any valid Python code can be used to control and extend
Topographica; documentation for Python and existing Topographica
commands can be accessed from the Help menu of the Topographica
Console window.</p>
<p>Please contact <a class="reference external" href="mailto:jbednar&#37;&#52;&#48;inf&#46;ed&#46;ac&#46;uk?subject=Comments%20on%20Topographica%20tutorial">jbednar<span>&#64;</span>inf<span>&#46;</span>ed<span>&#46;</span>ac<span>&#46;</span>uk</a> if you have questions or
suggestions about the software or this tutorial.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/topo-banner7.png" alt="Logo"/>
            </a></p>
<ul class="global-toc">
<li><a href="../index.html">Home</a></li>
<li><a href="../News/index.html">News</a></li>
<li><a href="../Downloads/index.html">Downloads</a></li>
<li><a href="index.html">Tutorials</a></li>
<li><a href="../User_Manual/index.html">User Manual</a></li>
<li><a href="../Reference_Manual/index.html">Reference Manual</a></li>
<li><a href="../Developer_Manual/index.html">Developer Manual</a></li>
<li><a href="../Forums/index.html">Forums</a></li>
<li><a href="../Team_Members/index.html">Team Members</a></li>
<li><a href="../Future_Work/index.html">Future Work</a></li>
<li><a href="../FAQ/index.html">FAQ</a></li>
<li><a href="../Links/index.html">Links</a></li>
<li><a href="../Home/pubs.html">Publications</a></li>
</ul>
<h3><a href="../index.html">Table Of Contents</a></h3>
<ul>
<li><a class="reference internal" href="#">LISSOM Orientation Map</a><ul>
<li><a class="reference internal" href="#response-of-an-orientation-map">Response of an orientation map</a></li>
<li><a class="reference internal" href="#learning-optional">Learning (optional)</a></li>
<li><a class="reference internal" href="#exploring-further">Exploring further</a></li>
</ul>
</li>
</ul>


  <h4>Previous topic</h4>
  <p class="topless"><a href="gcal.html"
                        title="previous chapter">GCAL Orientation Map</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="som_retinotopy.html"
                        title="next chapter">SOM Retinotopic Map</a></p>
<h3>This Page</h3>
<ul class="this-page-menu">
	<li><a	href="https://github.com/wiktr/topographica/edit/doc/doc/Tutorials/lissom_oo_or.rst" rel="nofollow">Edit on GitHub</a></li>
</ul>

<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="som_retinotopy.html" title="SOM Retinotopic Map"
             >next</a></li>
        <li class="right" >
          <a href="gcal.html" title="GCAL Orientation Map"
             >previous</a> |</li>
<li><a href="../index.html">Home</a></li>
<li><a href="../Downloads/index.html">Downloads</a></li>
<li><a href="index.html">Tutorials</a></li>
<li><a href="../User_Manual/index.html">User Manual</a></li>


<li><ul class="parents">



          <li><a href="index.html" >Tutorials</a> &raquo;</li>

</ul></li>


      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, IOAM.
      Last updated on May 09, 2013.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>